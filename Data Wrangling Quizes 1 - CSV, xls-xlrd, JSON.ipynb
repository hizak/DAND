{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Title', 'Released', 'Label', 'UK Chart Position', 'US Chart Position', 'BPI Certification', 'RIAA Certification']\n",
      "Please Please Me,22 March 1963,Parlophone(UK),1,-,Gold,Platinum\n",
      "\n",
      "['Please Please Me', '22 March 1963', 'Parlophone(UK)', '1', '-', 'Gold', 'Platinum\\n']\n",
      "0 Please Please Me\n",
      "1 22 March 1963\n",
      "2 Parlophone(UK)\n",
      "3 1\n",
      "4 -\n",
      "5 Gold\n",
      "6 Platinum\n",
      "\n",
      "With the Beatles,22 November 1963,Parlophone(UK),1,-,Platinum,Gold\n",
      "\n",
      "['With the Beatles', '22 November 1963', 'Parlophone(UK)', '1', '-', 'Platinum', 'Gold\\n']\n",
      "0 With the Beatles\n",
      "1 22 November 1963\n",
      "2 Parlophone(UK)\n",
      "3 1\n",
      "4 -\n",
      "5 Platinum\n",
      "6 Gold\n",
      "\n",
      "Beatlemania! With the Beatles,25 November 1963,Capitol(CAN),-,-,,\n",
      "\n",
      "['Beatlemania! With the Beatles', '25 November 1963', 'Capitol(CAN)', '-', '-', '', '\\n']\n",
      "0 Beatlemania! With the Beatles\n",
      "1 25 November 1963\n",
      "2 Capitol(CAN)\n",
      "3 -\n",
      "4 -\n",
      "5 \n",
      "6 \n",
      "\n",
      "Introducing... The Beatles,10 January 1964,Vee-Jay(US),-,2,,\n",
      "\n",
      "['Introducing... The Beatles', '10 January 1964', 'Vee-Jay(US)', '-', '2', '', '\\n']\n",
      "0 Introducing... The Beatles\n",
      "1 10 January 1964\n",
      "2 Vee-Jay(US)\n",
      "3 -\n",
      "4 2\n",
      "5 \n",
      "6 \n",
      "\n",
      "Meet the Beatles!,20 January 1964,Capitol(US),-,1,,5xPlatinum\n",
      "\n",
      "['Meet the Beatles!', '20 January 1964', 'Capitol(US)', '-', '1', '', '5xPlatinum\\n']\n",
      "0 Meet the Beatles!\n",
      "1 20 January 1964\n",
      "2 Capitol(US)\n",
      "3 -\n",
      "4 1\n",
      "5 \n",
      "6 5xPlatinum\n",
      "\n",
      "Twist and Shout,3 February 1964,Capitol(CAN),-,-,,\n",
      "\n",
      "['Twist and Shout', '3 February 1964', 'Capitol(CAN)', '-', '-', '', '\\n']\n",
      "0 Twist and Shout\n",
      "1 3 February 1964\n",
      "2 Capitol(CAN)\n",
      "3 -\n",
      "4 -\n",
      "5 \n",
      "6 \n",
      "\n",
      "The Beatles' Second Album,10 April 1964,Capitol(US),-,1,,2xPlatinum\n",
      "\n",
      "[\"The Beatles' Second Album\", '10 April 1964', 'Capitol(US)', '-', '1', '', '2xPlatinum\\n']\n",
      "0 The Beatles' Second Album\n",
      "1 10 April 1964\n",
      "2 Capitol(US)\n",
      "3 -\n",
      "4 1\n",
      "5 \n",
      "6 2xPlatinum\n",
      "\n",
      "The Beatles' Long Tall Sally,11 May 1964,Capitol(CAN),-,-,,\n",
      "\n",
      "[\"The Beatles' Long Tall Sally\", '11 May 1964', 'Capitol(CAN)', '-', '-', '', '\\n']\n",
      "0 The Beatles' Long Tall Sally\n",
      "1 11 May 1964\n",
      "2 Capitol(CAN)\n",
      "3 -\n",
      "4 -\n",
      "5 \n",
      "6 \n",
      "\n",
      "A Hard Day's Night,26 June 1964,United Artists(US)[C],-,1,,4xPlatinum\n",
      "\n",
      "[\"A Hard Day's Night\", '26 June 1964', 'United Artists(US)[C]', '-', '1', '', '4xPlatinum\\n']\n",
      "0 A Hard Day's Night\n",
      "1 26 June 1964\n",
      "2 United Artists(US)[C]\n",
      "3 -\n",
      "4 1\n",
      "5 \n",
      "6 4xPlatinum\n",
      "\n",
      ",10 July 1964,Parlophone(UK),1,-,Gold,\n",
      "\n",
      "['', '10 July 1964', 'Parlophone(UK)', '1', '-', 'Gold', '\\n']\n",
      "0 \n",
      "1 10 July 1964\n",
      "2 Parlophone(UK)\n",
      "3 1\n",
      "4 -\n",
      "5 Gold\n",
      "6 \n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Your task is to read the input DATAFILE line by line, and for the first 10 lines (not including the header)\n",
    "# split each line on \",\" and then for each line, create a dictionary\n",
    "# where the key is the header title of the field, and the value is the value of that field in the row.\n",
    "# The function parse_file should return a list of dictionaries,\n",
    "# each data line in the file being a single list entry.\n",
    "# Field names and values should not contain extra whitespace, like spaces or newline characters.\n",
    "# You can use the Python string method strip() to remove the extra whitespace.\n",
    "# You have to parse only the first 10 data lines in this exercise,\n",
    "# so the returned list should have 10 entries!\n",
    "#\n",
    "# This is my solution to this problem\n",
    "import os\n",
    "\n",
    "DATADIR = \"C:\\Users\\Zaki\\Documents\\Data Analyst Nanodegree - Udacity\\Mod4 Data Wrangling\\\\\"\n",
    "#DATAFILE = \"C:\\Users\\Zaki\\Documents\\Data Analyst Nanodegree - Udacity\\Mod4 Data Wrangling\\\\beatles-discography.csv\"\n",
    "DATAFILE = \"beatles-discography.csv\"\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    data = []\n",
    "    count = 0\n",
    "    with open(datafile, \"r\") as f:\n",
    "        for line in f:\n",
    "            entry = {}\n",
    "            if count == 0:\n",
    "                strip_line = line.strip()    # remove empty space on both sides\n",
    "                keys = strip_line.split(',')\n",
    "                print keys\n",
    "            else:\n",
    "                #for line_num,line in enumerate(lines):\n",
    "                #line = line.strip()    # remove empty space on both sides\n",
    "                fields = line.split(',')\n",
    "                print line\n",
    "                print fields\n",
    "            \n",
    "                for i in range(len(keys)):\n",
    "                    print i, fields[i]\n",
    "                    entry[keys[i]] = fields[i].strip()\n",
    "\n",
    "                data.append(entry)\n",
    "            \n",
    "            if count == 10:\n",
    "                break\n",
    "            count += 1\n",
    "\n",
    "    return data\n",
    "\n",
    "#print parse_file(DATADIR+DATAFILE)\n",
    "\n",
    "def test():\n",
    "    # a simple test of your implemetation\n",
    "    datafile = os.path.join(DATADIR, DATAFILE)\n",
    "    d = parse_file(datafile)\n",
    "    firstline = {'Title': 'Please Please Me', 'UK Chart Position': '1', 'Label': 'Parlophone(UK)', 'Released': '22 March 1963', 'US Chart Position': '-', 'RIAA Certification': 'Platinum', 'BPI Certification': 'Gold'}\n",
    "    tenthline = {'Title': '', 'UK Chart Position': '1', 'Label': 'Parlophone(UK)', 'Released': '10 July 1964', 'US Chart Position': '-', 'RIAA Certification': '', 'BPI Certification': 'Gold'}\n",
    "\n",
    "    assert d[0] == firstline\n",
    "    assert d[9] == tenthline\n",
    "\n",
    "    \n",
    "print test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Title', 'Released', 'Label', 'UK Chart Position', 'US Chart Position', 'BPI Certification', 'RIAA Certification\\n']\n",
      "['Please Please Me', '22 March 1963', 'Parlophone(UK)', '1', '-', 'Gold', 'Platinum\\n']\n",
      "0 Please Please Me\n",
      "1 22 March 1963\n",
      "2 Parlophone(UK)\n",
      "3 1\n",
      "4 -\n",
      "5 Gold\n",
      "6 Platinum\n",
      "\n",
      "['With the Beatles', '22 November 1963', 'Parlophone(UK)', '1', '-', 'Platinum', 'Gold\\n']\n",
      "0 With the Beatles\n",
      "1 22 November 1963\n",
      "2 Parlophone(UK)\n",
      "3 1\n",
      "4 -\n",
      "5 Platinum\n",
      "6 Gold\n",
      "\n",
      "['Beatlemania! With the Beatles', '25 November 1963', 'Capitol(CAN)', '-', '-', '', '\\n']\n",
      "0 Beatlemania! With the Beatles\n",
      "1 25 November 1963\n",
      "2 Capitol(CAN)\n",
      "3 -\n",
      "4 -\n",
      "5 \n",
      "6 \n",
      "\n",
      "['Introducing... The Beatles', '10 January 1964', 'Vee-Jay(US)', '-', '2', '', '\\n']\n",
      "0 Introducing... The Beatles\n",
      "1 10 January 1964\n",
      "2 Vee-Jay(US)\n",
      "3 -\n",
      "4 2\n",
      "5 \n",
      "6 \n",
      "\n",
      "['Meet the Beatles!', '20 January 1964', 'Capitol(US)', '-', '1', '', '5xPlatinum\\n']\n",
      "0 Meet the Beatles!\n",
      "1 20 January 1964\n",
      "2 Capitol(US)\n",
      "3 -\n",
      "4 1\n",
      "5 \n",
      "6 5xPlatinum\n",
      "\n",
      "['Twist and Shout', '3 February 1964', 'Capitol(CAN)', '-', '-', '', '\\n']\n",
      "0 Twist and Shout\n",
      "1 3 February 1964\n",
      "2 Capitol(CAN)\n",
      "3 -\n",
      "4 -\n",
      "5 \n",
      "6 \n",
      "\n",
      "[\"The Beatles' Second Album\", '10 April 1964', 'Capitol(US)', '-', '1', '', '2xPlatinum\\n']\n",
      "0 The Beatles' Second Album\n",
      "1 10 April 1964\n",
      "2 Capitol(US)\n",
      "3 -\n",
      "4 1\n",
      "5 \n",
      "6 2xPlatinum\n",
      "\n",
      "[\"The Beatles' Long Tall Sally\", '11 May 1964', 'Capitol(CAN)', '-', '-', '', '\\n']\n",
      "0 The Beatles' Long Tall Sally\n",
      "1 11 May 1964\n",
      "2 Capitol(CAN)\n",
      "3 -\n",
      "4 -\n",
      "5 \n",
      "6 \n",
      "\n",
      "[\"A Hard Day's Night\", '26 June 1964', 'United Artists(US)[C]', '-', '1', '', '4xPlatinum\\n']\n",
      "0 A Hard Day's Night\n",
      "1 26 June 1964\n",
      "2 United Artists(US)[C]\n",
      "3 -\n",
      "4 1\n",
      "5 \n",
      "6 4xPlatinum\n",
      "\n",
      "['', '10 July 1964', 'Parlophone(UK)', '1', '-', 'Gold', '\\n']\n",
      "0 \n",
      "1 10 July 1964\n",
      "2 Parlophone(UK)\n",
      "3 1\n",
      "4 -\n",
      "5 Gold\n",
      "6 \n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Your task is to read the input DATAFILE line by line, and for the first 10 lines (not including the header)\n",
    "# split each line on \",\" and then for each line, create a dictionary\n",
    "# where the key is the header title of the field, and the value is the value of that field in the row.\n",
    "# The function parse_file should return a list of dictionaries,\n",
    "# each data line in the file being a single list entry.\n",
    "# Field names and values should not contain extra whitespace, like spaces or newline characters.\n",
    "# You can use the Python string method strip() to remove the extra whitespace.\n",
    "# You have to parse only the first 10 data lines in this exercise,\n",
    "# so the returned list should have 10 entries!\n",
    "\n",
    "# This is the instructor solution ... same output as above code\n",
    "import os\n",
    "\n",
    "DATADIR = \"C:\\Users\\Zaki\\Documents\\Data Analyst Nanodegree - Udacity\\Mod4 Data Wrangling\\\\\"\n",
    "#DATAFILE = \"C:\\Users\\Zaki\\Documents\\Data Analyst Nanodegree - Udacity\\Mod4 Data Wrangling\\\\beatles-discography.csv\"\n",
    "DATAFILE = \"beatles-discography.csv\"\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    data = []\n",
    "    count = 0\n",
    "    with open(datafile, \"r\") as f:\n",
    "        header_line = f.readline().split(\",\")\n",
    "        print header_line\n",
    "        for line in f:\n",
    "            if count == 10:\n",
    "                break\n",
    "            entry = {}\n",
    "            fields = line.split(',')\n",
    "            print fields\n",
    "            \n",
    "            for i, value in enumerate(fields):\n",
    "                print i, value\n",
    "                entry[header_line[i].strip()] = value.strip()\n",
    "            data.append(entry)\n",
    "            \n",
    "            count += 1\n",
    "\n",
    "    return data\n",
    "\n",
    "#print parse_file(DATADIR+DATAFILE)\n",
    "\n",
    "def test():\n",
    "    # a simple test of your implemetation\n",
    "    datafile = os.path.join(DATADIR, DATAFILE)\n",
    "    d = parse_file(datafile)\n",
    "    firstline = {'Title': 'Please Please Me', 'UK Chart Position': '1', 'Label': 'Parlophone(UK)', 'Released': '22 March 1963', 'US Chart Position': '-', 'RIAA Certification': 'Platinum', 'BPI Certification': 'Gold'}\n",
    "    tenthline = {'Title': '', 'UK Chart Position': '1', 'Label': 'Parlophone(UK)', 'Released': '10 July 1964', 'US Chart Position': '-', 'RIAA Certification': '', 'BPI Certification': 'Gold'}\n",
    "\n",
    "    assert d[0] == firstline\n",
    "    assert d[9] == tenthline\n",
    "\n",
    "    \n",
    "print test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List Comprehension\n",
      "data[3][2]: 1036.088697\n",
      "\n",
      "Cells in a nested loop:\n",
      "41277.0833333 9238.73731 1438.20528 1565.442856 916.708348 14010.903488 3027.98334 6165.211119 1157.741663 37520.933404 \n",
      "ROWS, COLUMNS, and CELLS:\n",
      "Number of rows in the sheet: 7296\n",
      "Type of data in cell (row 3, col 2): 2\n",
      "Value in cell (row 3, col 2): 1036.088697\n",
      "Get a slice of values in column 3, from rows 1-3:\n",
      "[1411.7505669999982, 1403.4722870000019, 1395.053150000001]\n",
      "\n",
      "DATES:\n",
      "Type of data in cell (row 1, col 0): 3\n",
      "Time in Excel format: 41275.0416667\n",
      "Convert time to a Python datetime tuple, from the Excel float: (2013, 1, 1, 1, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "# Intro xlrd section\n",
    "# This code impors the excel read (xlrd) module and read an an excel format file (old .xls)\n",
    "\n",
    "import xlrd\n",
    "\n",
    "datafile = \"2013_ERCOT_Hourly_Load_Data.xls\"\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "\n",
    "    data = [[sheet.cell_value(r, col) \n",
    "                for col in range(sheet.ncols)] \n",
    "                    for r in range(sheet.nrows)]\n",
    "\n",
    "    print \"\\nList Comprehension\"\n",
    "    print \"data[3][2]:\",\n",
    "    print data[3][2]\n",
    "\n",
    "    print \"\\nCells in a nested loop:\"    \n",
    "    for row in range(sheet.nrows):\n",
    "        for col in range(sheet.ncols):\n",
    "            if row == 50:\n",
    "                print sheet.cell_value(row, col),\n",
    "\n",
    "\n",
    "    ### other useful methods:\n",
    "    print \"\\nROWS, COLUMNS, and CELLS:\"\n",
    "    print \"Number of rows in the sheet:\", \n",
    "    print sheet.nrows\n",
    "    print \"Type of data in cell (row 3, col 2):\", \n",
    "    print sheet.cell_type(3, 2)\n",
    "    print \"Value in cell (row 3, col 2):\", \n",
    "    print sheet.cell_value(3, 2)\n",
    "    print \"Get a slice of values in column 3, from rows 1-3:\"\n",
    "    print sheet.col_values(3, start_rowx=1, end_rowx=4)\n",
    "\n",
    "    print \"\\nDATES:\"\n",
    "    print \"Type of data in cell (row 1, col 0):\", \n",
    "    print sheet.cell_type(1, 0)\n",
    "    exceltime = sheet.cell_value(1, 0)\n",
    "    print \"Time in Excel format:\",\n",
    "    print exceltime\n",
    "    print \"Convert time to a Python datetime tuple, from the Excel float:\",\n",
    "    print xlrd.xldate_as_tuple(exceltime, 0)\n",
    "\n",
    "    return data\n",
    "\n",
    "data = parse_file(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7295 7295\n",
      "11194.485301 11194.485301\n",
      "10976.9334607 6602.113899 18779.02551\n",
      "795\n",
      "5391\n",
      "\n",
      "ROWS, COLUMNS, and CELLS:\n",
      "Number of rows in the sheet: 7296\n",
      "Type of data in cell (row 3, col 2): 2\n",
      "Value in cell (row 3, col 2): 1036.088697\n",
      "Get a slice of values in column 3, from rows 1-3:\n",
      "[1411.7505669999982, 1403.4722870000019, 1395.053150000001]\n",
      "\n",
      "Time in Excel format: for max value 41499.7083333\n",
      "Convert time to a Python datetime tuple, from the Excel float: (2013, 8, 13, 17, 0, 0)\n",
      "\n",
      "Time in Excel format: for min value 41308.125\n",
      "Convert time to a Python datetime tuple, from the Excel float: (2013, 2, 3, 3, 0, 0)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Your task is as follows:\n",
    "- read the provided Excel file\n",
    "- find and return the min, max and average values for the COAST region\n",
    "- find and return the time value for the min and max entries\n",
    "- the time values should be returned as Python tuples\n",
    "\n",
    "Please see the test function for the expected return format\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#This is my solution, next cell is instructor solution.....both give the correct output\n",
    "\n",
    "import xlrd\n",
    "from zipfile import ZipFile\n",
    "#DATADIR = \"C:\\Users\\Zaki\\Documents\\Data Analyst Nanodegree - Udacity\\Mod4 Data Wrangling\\\\\"\n",
    "zipfile = \"C:\\Users\\Zaki\\Documents\\Data Analyst Nanodegree - Udacity\\Mod4 Data Wrangling\\\\2013-ercot-hourly-load-data\"\n",
    "datafile = \"C:\\Users\\Zaki\\Documents\\Data Analyst Nanodegree - Udacity\\Mod4 Data Wrangling\\\\2013_ERCOT_Hourly_Load_Data.xls\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def open_zip(datafile):\n",
    "    with ZipFile('{0}.zip'.format(datafile), 'r') as myzip:\n",
    "        myzip.extractall()\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "\n",
    "    ### example on how you can get the data\n",
    "    #sheet_data = [[sheet.cell_value(r, col) for col in range(sheet.ncols)] for r in range(sheet.nrows)]\n",
    "    sheet_data = [[sheet.cell_value(r, col) \n",
    "                for col in range(sheet.ncols)] \n",
    "                    for r in range(sheet.nrows)]\n",
    "    \n",
    "    #print sheet_data\n",
    "    #print sheet_data[1][1]\n",
    "\n",
    "    #print \"\\nCells in a nested loop:\"    \n",
    "    #for row in range(sheet.nrows):\n",
    "    #    for col in range(sheet.ncols):\n",
    "    #        if row == 50:\n",
    "    #            print sheet.cell_value(row, col),\n",
    "    \n",
    "    coast_list=[]\n",
    "    #print \"\\nCells in a nested loop:\"    \n",
    "    for col in range(sheet.ncols):\n",
    "        for row in range(sheet.nrows):\n",
    "            if ((col == 1) & (row != 0)):\n",
    "                #print sheet.cell_value(row, col),\n",
    "                coast_list.append(sheet.cell_value(row, col))\n",
    "                #print sheet.cell_value(row, col)\n",
    "    #np_myarray = np.asarray(coast_list)\n",
    "    np_myarray = np.array(coast_list)\n",
    "\n",
    "    #print type(test)\n",
    "    print len(np_myarray), len(coast_list)\n",
    "    print np_myarray[3647], coast_list[3647] \n",
    "    #print np_myarray[5393]\n",
    "    #print np_myarray[5394]\n",
    "\n",
    "    print np_myarray.mean(),np_myarray.min(), np_myarray.max()\n",
    "    \n",
    "    # get thet index of the min and nax values, which is a tuple of the index and type\n",
    "    min_index = int(np.where(np_myarray == np_myarray.min())[0])\n",
    "    print min_index\n",
    "    \n",
    "    # get just the integer index of the where returned tuple\n",
    "    max_index = int(np.where(np_myarray == np_myarray.max())[0])\n",
    "    print max_index\n",
    "    \n",
    "    # test this is the correct index by print the value at gthe index\n",
    "    #print np_myarray[np.where(np_myarray == np_myarray.max())]\n",
    "    \n",
    "    #print np_array.argmax(np_array.min())\n",
    "    \n",
    "    maxvalue = np_myarray.max()\n",
    "    minvalue = np_myarray.min()\n",
    "    avevalue = np_myarray.mean() \n",
    "\n",
    "    ### other useful methods:\n",
    "    print \"\\nROWS, COLUMNS, and CELLS:\"\n",
    "    print \"Number of rows in the sheet:\", \n",
    "    print sheet.nrows\n",
    "    print \"Type of data in cell (row 3, col 2):\", \n",
    "    print sheet.cell_type(3, 2)\n",
    "    print \"Value in cell (row 3, col 2):\", \n",
    "    print sheet.cell_value(3, 2)\n",
    "    print \"Get a slice of values in column 3, from rows 1-3:\"\n",
    "    print sheet.col_values(3, start_rowx=1, end_rowx=4)\n",
    "\n",
    "    # print \"\\nDATES:\"\n",
    "    # print \"Type of data in cell (row 1, col 0):\", \n",
    "    # print sheet.cell_type(1, 0)\n",
    "    \n",
    "    #get the time value corresponding to the max value from the index at column 0, which is the date\n",
    "    # the correct index would be max_index +1 because we removed the heder line???? not sure why\n",
    "    exceltime = sheet.cell_value(max_index+1, 0)\n",
    "    print \"\\nTime in Excel format: for max value\",\n",
    "    print exceltime\n",
    "    print \"Convert time to a Python datetime tuple, from the Excel float:\",\n",
    "    maxtime = xlrd.xldate_as_tuple(exceltime, 0)\n",
    "    print maxtime\n",
    "    \n",
    "    #get the time value corresponding to the min value from the index at column 0, which is the date\n",
    "    exceltime = sheet.cell_value(min_index+1, 0)\n",
    "    print \"\\nTime in Excel format: for min value\",\n",
    "    print exceltime\n",
    "    print \"Convert time to a Python datetime tuple, from the Excel float:\",\n",
    "    mintime = xlrd.xldate_as_tuple(exceltime, 0)\n",
    "    print mintime\n",
    "    \n",
    "    \n",
    "    data = {\n",
    "            'maxtime': maxtime,\n",
    "            'maxvalue': maxvalue,\n",
    "            'mintime': mintime,\n",
    "            'minvalue': minvalue,\n",
    "            'avgcoast': avevalue\n",
    "    }\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def test():\n",
    "    #open_zip(zipfile)\n",
    "    data = parse_file(datafile)\n",
    "\n",
    "    assert data['maxtime'] == (2013, 8, 13, 17, 0, 0)\n",
    "    assert round(data['maxvalue'], 10) == round(18779.02551, 10)\n",
    "\n",
    "\n",
    "print test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avgcoast': 10976.933460679751,\n",
      " 'maxtime': (2013, 8, 13, 17, 0, 0),\n",
      " 'maxvalue': 18779.025510000003,\n",
      " 'mintime': (2013, 2, 3, 4, 0, 0),\n",
      " 'minvalue': 6602.113898999982}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Your task is as follows:\n",
    "- read the provided Excel file\n",
    "- find and return the min, max and average values for the COAST region\n",
    "- find and return the time value for the min and max entries\n",
    "- the time values should be returned as Python tuples\n",
    "\n",
    "Please see the test function for the expected return format\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#This is the instructor solution.....both give the correct output\n",
    "\n",
    "import xlrd\n",
    "from zipfile import ZipFile\n",
    "\n",
    "zipfile = \"C:\\Users\\Zaki\\Documents\\Data Analyst Nanodegree - Udacity\\Mod4 Data Wrangling\\\\2013-ercot-hourly-load-data\"\n",
    "datafile = \"C:\\Users\\Zaki\\Documents\\Data Analyst Nanodegree - Udacity\\Mod4 Data Wrangling\\\\2013_ERCOT_Hourly_Load_Data.xls\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def open_zip(datafile):\n",
    "    with ZipFile('{0}.zip'.format(datafile), 'r') as myzip:\n",
    "        myzip.extractall()\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "\n",
    "    ### example on how you can get the data\n",
    "    #sheet_data = [[sheet.cell_value(r, col) for col in range(sheet.ncols)] for r in range(sheet.nrows)]\n",
    "    data = [[sheet.cell_value(r, col) \n",
    "                for col in range(sheet.ncols)] \n",
    "                    for r in range(sheet.nrows)]\n",
    "    \n",
    "    # Get all the Coast values starting from col=1 and row=1 to skip the header line\n",
    "    # I dont beleive how simple this is compared to what I have done above!!!\n",
    "    cv = sheet.col_values(1, start_rowx=1, end_rowx=None)\n",
    "    \n",
    "    maxval = max(cv)\n",
    "    minval = min(cv)\n",
    "    aveval = sum(cv) / float(len(cv))\n",
    "    \n",
    "    # get the index and correct by adding 1 as the data starts on rows 0\n",
    "    maxpos = cv.index(maxval) + 1\n",
    "    minpos = cv.index(minval) + 1\n",
    "    \n",
    "    # get the time corresponding to max and min which are in column 0\n",
    "    # and then Convert time to a Python datetime tuple, from the Excel float\n",
    "\n",
    "    maxtime = sheet.cell_value(maxpos, 0)\n",
    "    realmaxtime = xlrd.xldate_as_tuple(maxtime, 0)\n",
    "    mintime = sheet.cell_value(minpos, 0)\n",
    "    realmintime = xlrd.xldate_as_tuple(mintime, 0)\n",
    "    \n",
    "    \n",
    "    data = {\n",
    "            'maxtime': realmaxtime,\n",
    "            'maxvalue': maxval,\n",
    "            'mintime': realmintime,\n",
    "            'minvalue': minval,\n",
    "            'avgcoast': aveval\n",
    "    }\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def test():\n",
    "    #open_zip(zipfile)\n",
    "    data = parse_file(datafile)\n",
    "    \n",
    "    #add these lines to plot to check\n",
    "    # use pprint as it prints it nicer and on multiple lines\n",
    "    import pprint\n",
    "    pprint.pprint(data)\n",
    "    \n",
    "\n",
    "    assert data['maxtime'] == (2013, 8, 13, 17, 0, 0)\n",
    "    assert round(data['maxvalue'], 10) == round(18779.02551, 10)\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requesting http://musicbrainz.org/ws/2/artist/?query=artist%3AOne+Direction&fmt=json\n",
      "{\n",
      "    \"artists\": [\n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"8a754a16-0027-3a29-b6d7-2b40ea0481ed\", \n",
      "                \"name\": \"United Kingdom\", \n",
      "                \"sort-name\": \"United Kingdom\"\n",
      "            }, \n",
      "            \"begin-area\": {\n",
      "                \"id\": \"f03d09b3-39dc-4083-afd6-159e3f0d462f\", \n",
      "                \"name\": \"London\", \n",
      "                \"sort-name\": \"London\"\n",
      "            }, \n",
      "            \"country\": \"GB\", \n",
      "            \"id\": \"1a425bbd-cca4-4b2c-aeb7-71cb176c828a\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"2010-07\", \n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"One Direction\", \n",
      "            \"score\": \"100\", \n",
      "            \"sort-name\": \"One Direction\", \n",
      "            \"tags\": [\n",
      "                {\n",
      "                    \"count\": 2, \n",
      "                    \"name\": \"pop\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"power pop\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"dance-pop\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"pop rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"folk pop\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 2, \n",
      "                    \"name\": \"boy band\"\n",
      "                }\n",
      "            ], \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"489ce91b-6658-3307-9877-795b68554c98\", \n",
      "                \"name\": \"United States\", \n",
      "                \"sort-name\": \"United States\"\n",
      "            }, \n",
      "            \"country\": \"US\", \n",
      "            \"disambiguation\": \"San Francisco band\", \n",
      "            \"id\": \"d3479e62-76ac-4aec-9f95-5b222d1e26b1\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Direction\", \n",
      "            \"score\": \"69\", \n",
      "            \"sort-name\": \"Direction\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"b2bc1294-77be-4c7b-af93-9868b83b1f34\", \n",
      "                \"name\": \"Auckland\", \n",
      "                \"sort-name\": \"Auckland\"\n",
      "            }, \n",
      "            \"disambiguation\": \"New Zealand punk?\", \n",
      "            \"id\": \"a234ebb0-0399-4253-baf6-2ae0abc9d8f2\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Direction\", \n",
      "            \"score\": \"47\", \n",
      "            \"sort-name\": \"Direction\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"ef1b7cc0-cd26-36f4-8ea0-04d9623786c7\", \n",
      "                \"name\": \"Netherlands\", \n",
      "                \"sort-name\": \"Netherlands\"\n",
      "            }, \n",
      "            \"begin-area\": {\n",
      "                \"id\": \"0b565bc6-05e7-4c3a-bc28-075fabd9a8d0\", \n",
      "                \"name\": \"Maastricht\", \n",
      "                \"sort-name\": \"Maastricht\"\n",
      "            }, \n",
      "            \"country\": \"NL\", \n",
      "            \"id\": \"574236dc-aa46-4194-8200-6cb039e9ddbd\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"1989\", \n",
      "                \"end\": \"2008\", \n",
      "                \"ended\": true\n",
      "            }, \n",
      "            \"name\": \"Right Direction\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"Right Direction\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"47813cd0-be78-48ab-802f-1306fc180124\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Loose Direction\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"Loose Direction\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"cfd2b787-3656-44ce-9212-9e555b400af5\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"New Direction\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"New Direction\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"2524b4ea-965f-4628-b940-acd0a9210047\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Lost Direction\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"Lost Direction\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"a32cc520-f2e6-4db2-8b61-c7baca599ef5\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Second Direction\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"Second Direction\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"75fb2c35-4f3a-4145-b948-da871176f039\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Disco Direction\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"Disco Direction\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"9e9d35c3-ce54-4a48-b3ba-4eebdadc1ed8\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Audio Direction\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"Audio Direction\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"489ce91b-6658-3307-9877-795b68554c98\", \n",
      "                \"name\": \"United States\", \n",
      "                \"sort-name\": \"United States\"\n",
      "            }, \n",
      "            \"country\": \"US\", \n",
      "            \"disambiguation\": \"USA punk band\", \n",
      "            \"id\": \"d554887e-2e17-48f4-8610-624a944a9329\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"No Direction\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"No Direction\", \n",
      "            \"tags\": [\n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"punk\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"usa\"\n",
      "                }\n",
      "            ], \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"6130ddae-22dd-4930-ba0f-6d59f81d3637\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"2009-08\", \n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"The Direction\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"Direction, The\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"355fa6a9-dba2-43b0-bd0f-a9e6dc14c79f\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Direction X\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"Direction X\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"74c2fbfc-0ced-4f8a-9da6-7df628755757\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"No Direction\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"No Direction\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"7c4e4dc0-6a0e-4887-b374-0cc57749f563\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Direction Indicator\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"Direction Indicator\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"ce85410c-10e4-4b2e-a878-72483fcf0884\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"7th Direction\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"7th Direction\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"65cd1bef-0de3-439e-a2fc-a71c308c6359\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"\\u9ad8\\u67f3\\u660c\\u884c New Direction\", \n",
      "            \"score\": \"38\", \n",
      "            \"sort-name\": \"Masayuki Takayanagi New Direction\"\n",
      "        }, \n",
      "        {\n",
      "            \"aliases\": [\n",
      "                {\n",
      "                    \"begin-date\": null, \n",
      "                    \"end-date\": null, \n",
      "                    \"locale\": null, \n",
      "                    \"name\": \"Masayuki \\\"Jojo\\\" Takayanagi* & New Direction For The Arts\", \n",
      "                    \"primary\": null, \n",
      "                    \"sort-name\": \"Masayuki \\\"Jojo\\\" Takayanagi* & New Direction For The Arts\", \n",
      "                    \"type\": null\n",
      "                }\n",
      "            ], \n",
      "            \"area\": {\n",
      "                \"id\": \"2db42837-c832-3c27-b4a3-08198f75693c\", \n",
      "                \"name\": \"Japan\", \n",
      "                \"sort-name\": \"Japan\"\n",
      "            }, \n",
      "            \"country\": \"JP\", \n",
      "            \"id\": \"8367123e-9a2b-4f1a-ad22-52dee85b144a\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"New Direction for the Arts\", \n",
      "            \"score\": \"36\", \n",
      "            \"sort-name\": \"New Direction for the Arts\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"3ccdd708-a6f2-4117-938d-4b23f045cbbb\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"The New Transit Direction\", \n",
      "            \"score\": \"34\", \n",
      "            \"sort-name\": \"New Transit Direction, The\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"85752fda-13c4-31a3-bee5-0e5cb1f51dad\", \n",
      "                \"name\": \"Germany\", \n",
      "                \"sort-name\": \"Germany\"\n",
      "            }, \n",
      "            \"country\": \"DE\", \n",
      "            \"disambiguation\": \"Hardcore/Gabber DJ Team\", \n",
      "            \"id\": \"06a2966d-95c1-4e20-a4d6-26e15d1d7e69\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"2010\", \n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Tensor & Re-Direction\", \n",
      "            \"score\": \"34\", \n",
      "            \"sort-name\": \"Tensor & Re-Direction\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"71bbafaa-e825-3e15-8ca9-017dcad1748b\", \n",
      "                \"name\": \"Canada\", \n",
      "                \"sort-name\": \"Canada\"\n",
      "            }, \n",
      "            \"country\": \"CA\", \n",
      "            \"id\": \"e0027978-441c-4877-b36f-0d2833181a42\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"The Beat Direction\", \n",
      "            \"score\": \"34\", \n",
      "            \"sort-name\": \"Beat Direction, The\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"disambiguation\": \"rock\", \n",
      "            \"id\": \"dffb4132-bfd1-4c23-8128-31c23d0b6fe9\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Found My Direction\", \n",
      "            \"score\": \"34\", \n",
      "            \"sort-name\": \"Found My Direction\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"106e0bec-b638-3b37-b731-f53d507dc00e\", \n",
      "                \"name\": \"Australia\", \n",
      "                \"sort-name\": \"Australia\"\n",
      "            }, \n",
      "            \"country\": \"AU\", \n",
      "            \"disambiguation\": \"Hardcore band from Australia\", \n",
      "            \"id\": \"6fe5e79d-c29a-4a14-91c2-f2e587cba2dd\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Found My Direction\", \n",
      "            \"score\": \"34\", \n",
      "            \"sort-name\": \"Found My Direction\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"982461f8-d0e0-44c0-9aa3-9ce9df28c71a\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Moving in The Right Direction\", \n",
      "            \"score\": \"31\", \n",
      "            \"sort-name\": \"Moving in The Right Direction\"\n",
      "        }, \n",
      "        {\n",
      "            \"disambiguation\": \"Church choir at Lourdes\", \n",
      "            \"id\": \"c2485094-3ac9-4632-b8d8-aacae7df3b6f\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Chorale et assembl\\u00e9e sous la direction de Jean Zune\", \n",
      "            \"score\": \"30\", \n",
      "            \"sort-name\": \"Chorale et assembl\\u00e9e sous la direction de Jean Zune\"\n",
      "        }\n",
      "    ], \n",
      "    \"count\": 1786, \n",
      "    \"created\": \"2017-08-22T16:44:54.263Z\", \n",
      "    \"offset\": 0\n",
      "}\n",
      "\n",
      "ARTIST:\n",
      "{\n",
      "    \"area\": {\n",
      "        \"id\": \"8a754a16-0027-3a29-b6d7-2b40ea0481ed\", \n",
      "        \"name\": \"United Kingdom\", \n",
      "        \"sort-name\": \"United Kingdom\"\n",
      "    }, \n",
      "    \"begin-area\": {\n",
      "        \"id\": \"f03d09b3-39dc-4083-afd6-159e3f0d462f\", \n",
      "        \"name\": \"London\", \n",
      "        \"sort-name\": \"London\"\n",
      "    }, \n",
      "    \"country\": \"GB\", \n",
      "    \"id\": \"1a425bbd-cca4-4b2c-aeb7-71cb176c828a\", \n",
      "    \"life-span\": {\n",
      "        \"begin\": \"2010-07\", \n",
      "        \"ended\": null\n",
      "    }, \n",
      "    \"name\": \"One Direction\", \n",
      "    \"score\": \"100\", \n",
      "    \"sort-name\": \"One Direction\", \n",
      "    \"tags\": [\n",
      "        {\n",
      "            \"count\": 2, \n",
      "            \"name\": \"pop\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"power pop\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"dance-pop\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"pop rock\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 1, \n",
      "            \"name\": \"folk pop\"\n",
      "        }, \n",
      "        {\n",
      "            \"count\": 2, \n",
      "            \"name\": \"boy band\"\n",
      "        }\n",
      "    ], \n",
      "    \"type\": \"Group\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requesting http://musicbrainz.org/ws/2/artist/1a425bbd-cca4-4b2c-aeb7-71cb176c828a?fmt=json&inc=releases\n",
      "\n",
      "ONE RELEASE:\n",
      "{\n",
      "  \"barcode\": null, \n",
      "  \"country\": \"GB\", \n",
      "  \"date\": \"2010-11-13\", \n",
      "  \"disambiguation\": \"\", \n",
      "  \"id\": \"0810df0f-304d-435c-884c-326e53577d33\", \n",
      "  \"packaging\": null, \n",
      "  \"packaging-id\": null, \n",
      "  \"quality\": \"normal\", \n",
      "  \"release-events\": [\n",
      "    {\n",
      "      \"area\": {\n",
      "        \"disambiguation\": \"\", \n",
      "        \"id\": \"8a754a16-0027-3a29-b6d7-2b40ea0481ed\", \n",
      "        \"iso-3166-1-codes\": [\n",
      "          \"GB\"\n",
      "        ], \n",
      "        \"name\": \"United Kingdom\", \n",
      "        \"sort-name\": \"United Kingdom\"\n",
      "      }, \n",
      "      \"date\": \"2010-11-13\"\n",
      "    }\n",
      "  ], \n",
      "  \"status\": \"Official\", \n",
      "  \"status-id\": \"4e304316-386d-3409-af2e-78857eec5cfe\", \n",
      "  \"text-representation\": {\n",
      "    \"language\": \"eng\", \n",
      "    \"script\": \"Latn\"\n",
      "  }, \n",
      "  \"title\": \"Something About the Way You Look Tonight\"\n",
      "}\n",
      "\n",
      "ALL TITLES:\n",
      "Something About the Way You Look Tonight\n",
      "Up All Night\n",
      "My Life Would Suck Without You\n",
      "Wishing on a Star\n",
      "Up All Night\n",
      "Kids in America\n",
      "Gotta Be You\n",
      "Gotta Be You\n",
      "Total Eclipse of the Heart\n",
      "What Makes You Beautiful\n",
      "Up All Night\n",
      "Your Song\n",
      "One Thing\n",
      "Up All Night\n",
      "Only Girl (in the World)\n",
      "Chasing Cars\n",
      "One Thing\n",
      "You Are So Beautiful\n",
      "All You Need Is Love\n",
      "What Makes You Beautiful\n",
      "Summer of '69\n",
      "Wishing on a Star\n",
      "Up All Night\n",
      "Nobody Knows\n",
      "Viva la vida\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "To experiment with this code freely you will have to run this code locally.\n",
    "Take a look at the main() function for an example of how to use the code. We\n",
    "have provided example json output in the other code editor tabs for you to look\n",
    "at, but you will not be able to run any queries through our UI.\n",
    "\"\"\"\n",
    "import json\n",
    "import requests\n",
    "\n",
    "BASE_URL = \"http://musicbrainz.org/ws/2/\"\n",
    "ARTIST_URL = BASE_URL + \"artist/\"\n",
    "\n",
    "\n",
    "# query parameters are given to the requests.get function as a dictionary; this\n",
    "# variable contains some starter parameters.\n",
    "query_type = {  \"simple\": {},\n",
    "                \"atr\": {\"inc\": \"aliases+tags+ratings\"},\n",
    "                \"aliases\": {\"inc\": \"aliases\"},\n",
    "                \"releases\": {\"inc\": \"releases\"}}\n",
    "\n",
    "\n",
    "def query_site(url, params, uid=\"\", fmt=\"json\"):\n",
    "    \"\"\"\n",
    "    This is the main function for making queries to the musicbrainz API. The\n",
    "    query should return a json document.\n",
    "    \"\"\"\n",
    "    params[\"fmt\"] = fmt\n",
    "    r = requests.get(url + uid, params=params)\n",
    "    print \"requesting\", r.url\n",
    "\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        return r.json()\n",
    "    else:\n",
    "        r.raise_for_status()\n",
    "\n",
    "\n",
    "def query_by_name(url, params, name):\n",
    "    \"\"\"\n",
    "    This adds an artist name to the query parameters before making an API call\n",
    "    to the function above.\n",
    "    \"\"\"\n",
    "    params[\"query\"] = \"artist:\" + name\n",
    "    return query_site(url, params)\n",
    "\n",
    "\n",
    "def pretty_print(data, indent=4):\n",
    "    \"\"\"\n",
    "    After we get our output, we can use this function to format it to be more\n",
    "    readable.\n",
    "    \"\"\"\n",
    "    if type(data) == dict:\n",
    "        print json.dumps(data, indent=indent, sort_keys=True)\n",
    "    else:\n",
    "        print data\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Below is an example investigation to help you get started in your\n",
    "    exploration. Modify the function calls and indexing below to answer the\n",
    "    questions on the next quiz.\n",
    "\n",
    "    HINT: Note how the output we get from the site is a multi-level JSON\n",
    "    document, so try making print statements to step through the structure one\n",
    "    level at a time or copy the output to a separate output file. Experimenting\n",
    "    and iteration will be key to understand the structure of the data!\n",
    "    \"\"\"\n",
    "\n",
    "    # Query for information in the database about bands named Nirvana\n",
    "    # Nirvana produces an error as a band called nirvana was added to the databses that had no releases!\n",
    "    #results = query_by_name(ARTIST_URL, query_type[\"simple\"], \"Nirvana\")   #incorrect results\n",
    "    #results = query_by_name(ARTIST_URL, query_type[\"simple\"], \"Bob Marley\")   # artisit index  = 1\n",
    "    #results = query_by_name(ARTIST_URL, query_type[\"simple\"], \"The Doors\")    # artist index = 0\n",
    "    #results = query_by_name(ARTIST_URL, query_type[\"simple\"], \"The Beatles\")    # artist index = 3\n",
    "    results = query_by_name(ARTIST_URL, query_type[\"simple\"], \"One Direction\")    # artist index = 3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    pretty_print(results)\n",
    "\n",
    "    # Isolate information from the 4th band returned (index 3)\n",
    "    print \"\\nARTIST:\"\n",
    "    pretty_print(results[\"artists\"][0])\n",
    "\n",
    "    # Query for releases from that band using the artist_id\n",
    "    #artist_id = results[\"artists\"][3][\"id\"]\n",
    "    artist_id = results[\"artists\"][0][\"id\"]\n",
    "    artist_data = query_site(ARTIST_URL, query_type[\"releases\"], artist_id)\n",
    "    releases = artist_data[\"releases\"]\n",
    "\n",
    "    # Print information about releases from the selected band\n",
    "    print \"\\nONE RELEASE:\"\n",
    "    pretty_print(releases[0], indent=2)\n",
    "\n",
    "    release_titles = [r[\"title\"] for r in releases]\n",
    "    print \"\\nALL TITLES:\"\n",
    "    for t in release_titles:\n",
    "        print t\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOUNTAIN VIEW MOFFETT FLD NAS\n",
      "<type 'str'> 01:00\n",
      "<type 'str'> 1/1/2005\n",
      "['1/1/2005', '01:00', '0', '0', '0', '2', '0', '0', '2', '0', '0', '2', '0', '0', '2', '0', '0', '2', '0', '0', '2', '0', '0', '2', '0', '3', 'E', '9', '3', 'E', '9', '8', 'A', '7', '6', 'A', '7', '87', 'A', '7', '1013', 'A', '7', '150', 'A', '7', '2.1', 'A', '7', '16100', 'A', '7', '77777', 'A', '7', '1.1', 'E', '8', '0.099', 'F', '8', '0.16', 'F', '8', '0', '1', 'A', '7']\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-248-d35e62797ec7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-248-d35e62797ec7>\u001b[0m in \u001b[0;36mtest\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"MOUNTAIN VIEW MOFFETT FLD NAS\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"01:00\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"01/01/2005\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"2\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Your task is to process the supplied file and use the csv module to extract data from it.\n",
    "The data comes from NREL (National Renewable Energy Laboratory) website. Each file\n",
    "contains information from one meteorological station, in particular - about amount of\n",
    "solar and wind energy for each hour of day.\n",
    "\n",
    "Note that the first line of the datafile is neither data entry, nor header. It is a line\n",
    "describing the data source. You should extract the name of the station from it.\n",
    "\n",
    "The data should be returned as a list of lists (not dictionaries).\n",
    "You can use the csv modules \"reader\" method to get data in such format.\n",
    "Another useful method is next() - to get the next line from the iterator.\n",
    "You should only change the parse_file function.\n",
    "\"\"\"\n",
    "import csv\n",
    "import os\n",
    "#import unicodecsv\n",
    "\n",
    "\n",
    "DATADIR = \"\"\n",
    "DATAFILE = \"745090V2.csv\"\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    name = \"\"\n",
    "    data = []\n",
    "    with open(datafile,'rb') as f:\n",
    "        #pass\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "    \n",
    "        first_line = f.next().split(\",\")\n",
    "        name = first_line[1]\n",
    "        header = f.next().split(\",\")\n",
    "        \n",
    "        #reader = unicodecsv.DictReader(f)\n",
    "        #data = list(reader)\n",
    "\n",
    "        for row in reader:\n",
    "            data.append(row)\n",
    "            #print type(row), row\n",
    "            \n",
    "        # read all the file entries/rows\n",
    "        #reader = unicodecsv.DictReader(f)\n",
    "        #data = list(reader)\n",
    "        \n",
    "        #data= next(in_reader)\n",
    "        #data.append(datarow)\n",
    "\n",
    "    # Do not change the line below\n",
    "    return (name, data)\n",
    "\n",
    "\n",
    "def test():\n",
    "    datafile = os.path.join(DATADIR, DATAFILE)\n",
    "    name, data = parse_file(datafile)\n",
    "    \n",
    "\n",
    "    print name\n",
    "    print type(data[0][1]), data[0][1] \n",
    "    print type(data[2][0]), data[2][0]\n",
    "    #data[2][0] = \"01/01/2005\"\n",
    "    #data[2][0] = str(data[2][0])\n",
    "    if data[2][0] == \"01/01/2005\":\n",
    "        print \"Okkkkkkkkkkk\"\n",
    "    print data[0]\n",
    "\n",
    "    # could not get this to work as excel removes leading zeros\n",
    "    # I reformatted exel as text to keep leading zeros...that worked \n",
    "    # here i dont have a problem with name field, in online test runs there was a problem with it...,nt sure what is going on!!!\n",
    "    assert name == \"MOUNTAIN VIEW MOFFETT FLD NAS\"\n",
    "    assert data[0][1] == \"01:00\"\n",
    "    assert data[2][0] == \"01/01/2005\"\n",
    "    assert data[2][5] == \"2\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "Find the time and value of max load for each of the regions\n",
    "COAST, EAST, FAR_WEST, NORTH, NORTH_C, SOUTHERN, SOUTH_C, WEST\n",
    "and write the result out in a csv file, using pipe character | as the delimiter.\n",
    "\n",
    "An example output can be seen in the \"example.csv\" file.\n",
    "'''\n",
    "\n",
    "import xlrd\n",
    "import os\n",
    "import csv\n",
    "from zipfile import ZipFile\n",
    "\n",
    "datafile = \"2013_ERCOT_Hourly_Load_Data.xls\"\n",
    "outfile = \"2013_Max_Loads.csv\"\n",
    "zipfile = \"2013-ercot-hourly-load-data\"\n",
    "\n",
    "def open_zip(datafile):\n",
    "    with ZipFile('{0}.zip'.format(datafile), 'r') as myzip:\n",
    "        myzip.extractall()\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "    #data = None\n",
    "    data = {}\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # Remember that you can use xlrd.xldate_as_tuple(sometime, 0) to convert\n",
    "    # Excel date to Python tuple of (year, month, day, hour, minute, second)\n",
    "\n",
    "    # process all columns which contain station data\n",
    "    # Get all the Coast values starting from col=1 and row=1 to skip the header line\n",
    "    for col in range (1, 9):\n",
    "        station = sheet.cell_value(0, col)\n",
    "        cv = sheet.col_values(col, start_rowx=1, end_rowx=None)\n",
    "        \n",
    "        # get the maxval of area\n",
    "        maxval = max(cv)\n",
    "        # get the index and correct by adding 1 as the data starts on rows 0\n",
    "        maxpos = cv.index(maxval) + 1\n",
    "        # get the time corresponding to max from column 0\n",
    "        # and then Convert time to a Python datetime tuple, from the Excel float\n",
    "        maxtime = sheet.cell_value(maxpos, 0)\n",
    "        realtime = xlrd.xldate_as_tuple(maxtime, 0)\n",
    "        data[station] = {\"maxval\": maxval,\n",
    "                         \"maxtime\": realtime}\n",
    "    \n",
    "    return data\n",
    "\n",
    "def save_file(data, filename):\n",
    "    # YOUR CODE HERE\n",
    "    with open(filename, \"w\") as f:\n",
    "        w = csv.writer(f, delimiter='|')\n",
    "        w.writerow([\"Station\", \"Year\", \"Month\", \"Day\", \"Hour\", \"Max Load\"])\n",
    "        for station in data:\n",
    "            year, month, day, hour, _ , _= data[station][\"maxtime\"]\n",
    "            w.writerow([station, year, month, day, hour, data[station][\"maxval\"]])\n",
    "    \n",
    "def test():\n",
    "    open_zip(zipfile)\n",
    "    data = parse_file(datafile)\n",
    "    save_file(data, outfile)\n",
    "\n",
    "    number_of_rows = 0\n",
    "    stations = []\n",
    "\n",
    "    ans = {'FAR_WEST': {'Max Load': '2281.2722140000024',\n",
    "                        'Year': '2013',\n",
    "                        'Month': '6',\n",
    "                        'Day': '26',\n",
    "                        'Hour': '17'}}\n",
    "    correct_stations = ['COAST', 'EAST', 'FAR_WEST', 'NORTH',\n",
    "                        'NORTH_C', 'SOUTHERN', 'SOUTH_C', 'WEST']\n",
    "    fields = ['Year', 'Month', 'Day', 'Hour', 'Max Load']\n",
    "\n",
    "    with open(outfile) as of:\n",
    "        csvfile = csv.DictReader(of, delimiter=\"|\")\n",
    "        for line in csvfile:\n",
    "            station = line['Station']\n",
    "            if station == 'FAR_WEST':\n",
    "                for field in fields:\n",
    "                    # Check if 'Max Load' is within .1 of answer\n",
    "                    if field == 'Max Load':\n",
    "                        max_answer = round(float(ans[station][field]), 1)\n",
    "                        max_line = round(float(line[field]), 1)\n",
    "                        assert max_answer == max_line\n",
    "\n",
    "                    # Otherwise check for equality\n",
    "                    else:\n",
    "                        assert ans[station][field] == line[field]\n",
    "\n",
    "            number_of_rows += 1\n",
    "            stations.append(station)\n",
    "\n",
    "        # Output should be 8 lines not including header\n",
    "        assert number_of_rows == 8\n",
    "\n",
    "        # Check Station Names\n",
    "        assert set(stations) == set(correct_stations)\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    test()\n",
    "\n",
    "    #data = parse_file(datafile)\n",
    "    #print data\n",
    "    #save_file(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'FAR_WEST': {'maxval': 2281.2722140000024, 'maxtime': (2013, 6, 26, 17, 0, 0)}, u'NORTH': {'maxval': 1544.7707140000005, 'maxtime': (2013, 8, 7, 17, 0, 0)}, u'WEST': {'maxval': 1862.6137649999998, 'maxtime': (2013, 8, 7, 17, 0, 0)}, u'SOUTHERN': {'maxval': 5494.157645, 'maxtime': (2013, 8, 8, 16, 0, 0)}, u'SOUTH_C': {'maxval': 11433.30491600001, 'maxtime': (2013, 8, 8, 18, 0, 0)}, u'COAST': {'maxval': 18779.025510000003, 'maxtime': (2013, 8, 13, 17, 0, 0)}, u'NORTH_C': {'maxval': 24415.570226999993, 'maxtime': (2013, 8, 7, 18, 0, 0)}, u'EAST': {'maxval': 2380.1654089999956, 'maxtime': (2013, 8, 5, 17, 0, 0)}}\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "Find the time and value of max load for each of the regions\n",
    "COAST, EAST, FAR_WEST, NORTH, NORTH_C, SOUTHERN, SOUTH_C, WEST\n",
    "and write the result out in a csv file, using pipe character | as the delimiter.\n",
    "\n",
    "An example output can be seen in the \"example.csv\" file.\n",
    "'''\n",
    "\n",
    "# Instructor solution\n",
    "\n",
    "\n",
    "import xlrd\n",
    "import os\n",
    "import csv\n",
    "from zipfile import ZipFile\n",
    "\n",
    "datafile = \"2013_ERCOT_Hourly_Load_Data.xls\"\n",
    "outfile = \"2013_Max_Loads.csv\"\n",
    "\n",
    "def open_zip(datafile):\n",
    "    with ZipFile('{0}.zip'.format(datafile), 'r') as myzip:\n",
    "        myzip.extractall()\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "    data = {}\n",
    "    # process all rows that contain station data\n",
    "    for n in range (1, 9):\n",
    "        station = sheet.cell_value(0, n)\n",
    "        cv = sheet.col_values(n, start_rowx=1, end_rowx=None)\n",
    "\n",
    "        maxval = max(cv)\n",
    "        maxpos = cv.index(maxval) + 1\n",
    "        maxtime = sheet.cell_value(maxpos, 0)\n",
    "        realtime = xlrd.xldate_as_tuple(maxtime, 0)\n",
    "        data[station] = {\"maxval\": maxval,\n",
    "                         \"maxtime\": realtime}\n",
    "\n",
    "    return data\n",
    "\n",
    "def save_file(data, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        w = csv.writer(f, delimiter='|')\n",
    "        w.writerow([\"Station\", \"Year\", \"Month\", \"Day\", \"Hour\", \"Max Load\"])\n",
    "        for s in data:\n",
    "            year, month, day, hour, _ , _= data[s][\"maxtime\"]\n",
    "            w.writerow([s, year, month, day, hour, data[s][\"maxval\"]])\n",
    "            \n",
    "def test():\n",
    "    open_zip(datafile)\n",
    "    data = parse_file(datafile)\n",
    "    save_file(data, outfile)\n",
    "\n",
    "    number_of_rows = 0\n",
    "    stations = []\n",
    "\n",
    "    ans = {'FAR_WEST': {'Max Load': '2281.2722140000024',\n",
    "                        'Year': '2013',\n",
    "                        'Month': '6',\n",
    "                        'Day': '26',\n",
    "                        'Hour': '17'}}\n",
    "    correct_stations = ['COAST', 'EAST', 'FAR_WEST', 'NORTH',\n",
    "                        'NORTH_C', 'SOUTHERN', 'SOUTH_C', 'WEST']\n",
    "    fields = ['Year', 'Month', 'Day', 'Hour', 'Max Load']\n",
    "\n",
    "    with open(outfile) as of:\n",
    "        csvfile = csv.DictReader(of, delimiter=\"|\")\n",
    "        for line in csvfile:\n",
    "            station = line['Station']\n",
    "            if station == 'FAR_WEST':\n",
    "                for field in fields:\n",
    "                    # Check if 'Max Load' is within .1 of answer\n",
    "                    if field == 'Max Load':\n",
    "                        max_answer = round(float(ans[station][field]), 1)\n",
    "                        max_line = round(float(line[field]), 1)\n",
    "                        assert max_answer == max_line\n",
    "\n",
    "                    # Otherwise check for equality\n",
    "                    else:\n",
    "                        assert ans[station][field] == line[field]\n",
    "\n",
    "            number_of_rows += 1\n",
    "            stations.append(station)\n",
    "\n",
    "        # Output should be 8 lines not including header\n",
    "        assert number_of_rows == 8\n",
    "\n",
    "        # Check Station Names\n",
    "        assert set(stations) == set(correct_stations)\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "#    test()\n",
    "\n",
    "    data = parse_file(datafile)\n",
    "    print data\n",
    "    save_file(data, outfile)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This exercise shows some important concepts that you should be aware about:\n",
    "- using codecs module to write unicode files\n",
    "- using authentication with web APIs\n",
    "- using offset when accessing web APIs\n",
    "\n",
    "To run this code locally you have to register at the NYTimes developer site \n",
    "and get your own API key. You will be able to complete this exercise in our UI\n",
    "without doing so, as we have provided a sample result. (See the file \n",
    "'popular-viewed-1.json' from the tabs above.)\n",
    "\n",
    "Your task is to modify the article_overview() function to process the saved\n",
    "file that represents the most popular articles (by view count) from the last\n",
    "day, and return a tuple of variables containing the following data:\n",
    "- labels: list of dictionaries, where the keys are the \"section\" values and\n",
    "  values are the \"title\" values for each of the retrieved articles.\n",
    "- urls: list of URLs for all 'media' entries with \"format\": \"Standard Thumbnail\"\n",
    "\n",
    "All your changes should be in the article_overview() function. See the test() \n",
    "function for examples of the elements of the output lists.\n",
    "The rest of functions are provided for your convenience, if you want to access\n",
    "the API by yourself.\n",
    "\"\"\"\n",
    "import json\n",
    "import codecs\n",
    "import requests\n",
    "\n",
    "URL_MAIN = \"http://api.nytimes.com/svc/\"\n",
    "URL_POPULAR = URL_MAIN + \"mostpopular/v2/\"\n",
    "API_KEY = { \"popular\": \"\",\n",
    "            \"article\": \"\"}\n",
    "\n",
    "\n",
    "def get_from_file(kind, period):\n",
    "    filename = \"popular-{0}-{1}.json\".format(kind, period)\n",
    "    with open(filename, \"r\") as f:\n",
    "        return json.loads(f.read())\n",
    "\n",
    "\n",
    "def article_overview(kind, period):\n",
    "    data = get_from_file(kind, period)\n",
    "    titles = []\n",
    "    urls =[]\n",
    "\n",
    "    for article_obj in data:\n",
    "        entry = {}\n",
    "        #print \"\\n\", article_obj[\"views\"]\n",
    "        title =  article_obj[\"title\"].encode('utf-8')\n",
    "        section = article_obj[\"section\"]\n",
    "        entry = {section: title}\n",
    "        titles.append(entry)\n",
    "        \n",
    "        # this code did not work but can be tweaked to work\n",
    "        '''\n",
    "        if article_obj[\"media\"] != \"\":\n",
    "            for media_data_obj in article_obj[\"media\"][0][\"media-metadata\"]:\n",
    "                #format = article_obj[\"media\"][0][\"media-metadata\"][0][\"format\"].encode('utf-8')\n",
    "                format = media_data_obj[\"format\"]\n",
    "                if format == \"Standard Thumbnail\":\n",
    "                    urls.append(media_data_obj[\"url\"])\n",
    "        '''            \n",
    "        if article_obj[\"media\"] != \"\":\n",
    "        #if \"media\" in article_obj      # does the same as line above\n",
    "            for media_obj in article_obj[\"media\"]:\n",
    "                for media_meta_obj in media_obj[\"media-metadata\"]:\n",
    "                    format = media_meta_obj[\"format\"]\n",
    "                    if format == \"Standard Thumbnail\":\n",
    "                        urls.append(media_meta_obj[\"url\"])\n",
    "                    \n",
    "    #print titles\n",
    "    #print len(urls)\n",
    "    #print urls\n",
    "\n",
    "    return (titles, urls)\n",
    "\n",
    "\n",
    "def query_site(url, target, offset):\n",
    "    # This will set up the query with the API key and offset\n",
    "    # Web services often use offset paramter to return data in small chunks\n",
    "    # NYTimes returns 20 articles per request, if you want the next 20\n",
    "    # You have to provide the offset parameter\n",
    "    if API_KEY[\"popular\"] == \"\" or API_KEY[\"article\"] == \"\":\n",
    "        print \"You need to register for NYTimes Developer account to run this program.\"\n",
    "        print \"See Intructor notes for information\"\n",
    "        return False\n",
    "    params = {\"api-key\": API_KEY[target], \"offset\": offset}\n",
    "    r = requests.get(url, params = params)\n",
    "\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        return r.json()\n",
    "    else:\n",
    "        r.raise_for_status()\n",
    "\n",
    "\n",
    "def get_popular(url, kind, days, section=\"all-sections\", offset=0):\n",
    "    # This function will construct the query according to the requirements of the site\n",
    "    # and return the data, or print an error message if called incorrectly\n",
    "    if days not in [1,7,30]:\n",
    "        print \"Time period can be 1,7, 30 days only\"\n",
    "        return False\n",
    "    if kind not in [\"viewed\", \"shared\", \"emailed\"]:\n",
    "        print \"kind can be only one of viewed/shared/emailed\"\n",
    "        return False\n",
    "\n",
    "    url += \"most{0}/{1}/{2}.json\".format(kind, section, days)\n",
    "    data = query_site(url, \"popular\", offset)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_file(kind, period):\n",
    "    # This will process all results, by calling the API repeatedly with supplied offset value,\n",
    "    # combine the data and then write all results in a file.\n",
    "    data = get_popular(URL_POPULAR, \"viewed\", 1)\n",
    "    num_results = data[\"num_results\"]\n",
    "    full_data = []\n",
    "    with codecs.open(\"popular-{0}-{1}.json\".format(kind, period), encoding='utf-8', mode='w') as v:\n",
    "        for offset in range(0, num_results, 20):        \n",
    "            data = get_popular(URL_POPULAR, kind, period, offset=offset)\n",
    "            full_data += data[\"results\"]\n",
    "        \n",
    "        v.write(json.dumps(full_data, indent=2))\n",
    "\n",
    "\n",
    "def test():\n",
    "    titles, urls = article_overview(\"viewed\", 1)\n",
    "    assert len(titles) == 20\n",
    "    assert len(urls) == 30\n",
    "    assert titles[2] == {'Opinion': 'Professors, We Need You!'}\n",
    "    assert urls[20] == 'http://graphics8.nytimes.com/images/2014/02/17/sports/ICEDANCE/ICEDANCE-thumbStandard.jpg'\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
